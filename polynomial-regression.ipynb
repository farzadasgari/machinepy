{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f358d6-f54f-4d0f-b6c8-825f9a0f4e75",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a87d08-127c-4df8-bd82-7a5b7936c59d",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5706cc7-c4c4-4b8e-9ad2-298089a47009",
   "metadata": {},
   "source": [
    "This notebook provides an in-depth exploration of polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3f4bb-7ddd-4537-843b-368c85ec7794",
   "metadata": {},
   "source": [
    "**Polynomial Regression:** An extension of linear regression that captures non-linear relationships between features and target variables.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Simple linear regression assumes a linear relationship between the features and the target variable. However, in many real-world scenarios, the relationship might be more complex and non-linear.\n",
    "\n",
    "Polynomial regression addresses this limitation by introducing polynomial terms of the feature variable(s). These terms allow the model to capture curvature or bends in the data, leading to a better fit for non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f0435-45cf-4bde-998d-39792d18a902",
   "metadata": {},
   "source": [
    "The content herein covers the theoretical underpinning of polynomial regression, its practical implementation in `Python` leveraging prominent libraries such as `NumPy` and `SciKit-Learn`, and methodologies for assessing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344c506-5281-4eca-9a1d-f6f7ecfbbe76",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29a9cd-dcc3-4e74-bdd9-9a1009199472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bdb1ad5-3989-4c9d-a206-168cb22a16a1",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48536b00-987b-4538-b890-23da4e3c52d4",
   "metadata": {},
   "source": [
    "The dataset stored in a comma-seperated values (CSV) file is loaded using `pandas.read_csv()`. Pandas, a library offering a powerful suite of tools for data manipulation and analysis, is leveraged here. DataFrames, its core data structure, provide a tabular format for efficient data handling, making exploration, analysis, and visualization straightforwar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f285236c-8460-4e65-9ce0-e1542d3b86d4",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- The path should be modified if it's located elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e75db7-7695-47cb-95cf-aa84044d10b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0c07eb-7a89-4d98-b1a3-082e29fb3d2c",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39754ef2-f2a2-418f-bb5a-0d3ce1ab36ae",
   "metadata": {},
   "source": [
    "This section delves into the loaded dataset using various techniques from `pandas` and `seaborn` libraries to gain a comprehensive understanding of its characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e9b3e-125b-43f9-826c-a9a0e818999b",
   "metadata": {},
   "source": [
    "**1. Data Shape and Data Types:**\n",
    "   - The `data.shape` attribute is use to retrieve the dimension (number of rows and columns) of the `DataFrame`. This provides a quick overview of the data's size.\n",
    "   - The `data.dtypes` attribute returns a Series displaying the data type of each column. This helps identify potential data type mismatches or areas requiring type conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7cc31-5d0c-4bbf-91d8-f3e875ac0a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10abcd6-fee7-4811-a0b7-270b09189c39",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Handling Missing Values in *horsepower* column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92245e46-259e-498a-aaef-1636dfff163a",
   "metadata": {},
   "source": [
    "This section addresses the presence of missing values represented by the character `?` in the `horsepower` column of the dataset. Several steps are undertaken to ensure data quality and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d04f7-9939-4a2d-ad54-1d26746f64d2",
   "metadata": {},
   "source": [
    "**Identification of Non-Numeric Values:**\n",
    "The `np.nonzero(~data.horsepower.str.isdigit())[0]` expression leverages `NumPy` to identify the indices of rows within the `horsepower` column that contain non-numeric values (using string comparison with `str.isdigit()` and negation with tilde sign `~`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a6a5-01cf-4fe8-a9a8-fdf660c0be29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d02771d-e3ef-42b8-8f8b-c22e1077f286",
   "metadata": {},
   "source": [
    "The `data = data.replace('?', np.nan)` employs the replace method of the DataFrame to substitute all occurrences of the `?` character with the missing value representation `np.nan` (Not a Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fa603-af9c-433b-baf5-53329a38462a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81194db7-a4ea-49a2-88ef-c80bae0ac05c",
   "metadata": {},
   "source": [
    "**Verification of Missing Values (Optional):**\n",
    "The Line `data.isnull().sum()` utilizes the `isnull().sum()` method to display the total number of missing values present in each column after the replacement step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fc706-f80a-4cf7-8c39-7631bcb81972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e57b426e-120f-4b1a-92fe-0d2043e33f12",
   "metadata": {},
   "source": [
    "**Imputation with Median:**\n",
    "The core imputation step is implemented in `data['horsepower'] = data['horsepower'].fillna(data['horsepower'].astype('float64').median())`. Here, the `fillna()` method is applied to the `horsepower` column to fill missing values (represented by `NaN`). The median value is calculated using `data['horsepower'].astype('float64').median()`. Converting the column to numeric data type `(float64)` ensures proper calculation of the median. This approach replaces missing values with the central tendency of the existing numerical data within the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d14dd9-3f20-4eb0-a188-ca2eab56e498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec6eb0e1-7a63-4241-a8d6-5dee0f17dc07",
   "metadata": {},
   "source": [
    "**2. Data Skewness:**\n",
    "   - The `data.skew()` method calculates the skewness for each numerical column in the DataFrame. Skewness is a measure if the asymmetry of a distribution relative to a normal distribution. A positive skew indicates a distribution with a longer tail on the right, while a negative skew indicates a longer tail on the left. Understanding skewness can be crucial for selecting appropriate statistical methods or data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083298-ac44-449b-af86-c5daf5f6adbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315592eb-9f28-49ab-8727-a5ee5ea351d0",
   "metadata": {},
   "source": [
    "**3. Histogram Visualization:**\n",
    "   - The `data.hist()` method generates histograms for all numerical columns in the DataFrame. Histograms provides a visual representation of the dirstribution of data points, allowing you to identify potential patterns or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d268d98-afac-4919-944e-5758ac27a70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e1140b6-08e3-44d6-af4e-5814f10be25b",
   "metadata": {},
   "source": [
    "**4. Density Plot:**\n",
    "   - The `data.plot(kind='density')` method utilizes the `pandas.plotting` module to create density plots for all numerical columns. Density plot are similar to histograms, but depict the probability density of the data, offering a smoother visualization of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d84173-550b-4206-8064-6acdbf86debd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8ceb62-ebce-4adf-9318-f378bd87226f",
   "metadata": {},
   "source": [
    "**5. Box Plot:**\n",
    "   - The `data.plot(kind='box')` code leverages `pandas.plottong` to generate box plots for all numerical columns. Box plots display the distribution of data points using quartiles (25<sup>th</sup>, 50<sup>th</sup>, and 75<sup>th</sup> percentiles) and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad02d2c-35f8-473a-b3bd-8bf8f93e78ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cdc7528-da80-4a5a-b4e5-36953a457a19",
   "metadata": {},
   "source": [
    "**6. Correlation Heatmap:**\n",
    "   - The `sns.heatmap(data.corr(), annot=True)` code from the seaborn library creates a correlation heatmap to visualize the pairwise correlations between all numerical columns in the DataFrame. The correlation coefficient measure the strength and direction of the linear relationship between two variables. The heatmap provides a color-coded representation of these correlations, allowing you to identify potential relationships between features that might be relevant for you analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18bd63-934d-46cd-a121-81b460c7e2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5dfff6-fb6c-49b9-b84b-e3db1f4fdff1",
   "metadata": {},
   "source": [
    "**7. Scatter Matrix:**\n",
    "   - The `pd.plotting.scatter_matrix(data)` code from `pandas.plotting` module generates a scatter matrix, displaying all pairwise scatter plots between numerical columns in the DataFrame. Scatter plots visually depict the relationship between two variable, helping to identify potential linear or non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694c882-2506-4051-b507-8351d5e5763d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26b16080-24af-40c1-9661-3e8e5b20482f",
   "metadata": {},
   "source": [
    "**8. Pairplot with KDE:**\n",
    "   - The `sns.pairplot(data, diag_kind='kde')` code from seaborn craetes a pairplot, similar `scatter_matrix` of `pandas` but with the diagonal plots replaced by kernel density estimation (KDE) plots. KDE plots are a smooth representation of the distribution of each variable. This visualization provides a more comprehensive view of the relationships between features and the distribution of individual variables.\n",
    "\n",
    "By emplying these diverse exploratory data analysis techniques, you can gain valuable insights into the characteristics of your dataset, identify potential issues, and guide ffor further analysis or modelling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a6f39-6e8f-406d-ab45-afb76adda044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab443ad5-71dc-4caa-946d-eca4ec40070e",
   "metadata": {},
   "source": [
    "## Data Preparation for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a85dcf-735d-4a86-82f3-4851668886c0",
   "metadata": {},
   "source": [
    "This section focuses on preparing the data for simple linear regression analysis. Here, the target variable and the feature variable are isolated, and the data is subsequently split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae68254-bfb1-44b8-9d2f-9424748e44fd",
   "metadata": {},
   "source": [
    "**1. Target and Feature Variable Selection**\n",
    "- The target variable, representing the values to be predicted, is assigned to `y` by selecting the `mpg` (miles per gallon) column from the DataFrame `data`.\n",
    "- The feature variable, considered to influence the target variable, is assigned to `x` by selecting the `weight` column from the DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc4b7a-3d6a-47ef-b8a4-969308461972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f460f16b-4b22-4c43-9968-cb687835319a",
   "metadata": {},
   "source": [
    "**2. Train / Test Split**\n",
    "- The `train_test_split()` function from `scikit-learn` is employed to partition the data into training and testing sets. The split ensures a more robust evaluation of the model's performance, as the model is not directly exposed to the test data during training.\n",
    "- The arguments are:\n",
    "  - `x`: Feature variable (`weight`)\n",
    "  - `y`: Target variable(`mpg`)\n",
    "  - `test_size`: Allocates 20% of the data to the testing set and the remaining 80% to the training set.\n",
    "  - `random_state`: Sets a seed for random number generatorm ensuring reproducibility  of the split if code is run multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48547465-f1b2-47c7-b826-fd5249725865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217d4c25-d83f-412e-84e0-1af67ae4e432",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9c4bc-6968-40ba-b3bd-55656807e4f1",
   "metadata": {},
   "source": [
    "Here's the revised text in markdown format:\n",
    "\n",
    "## Polynomial Regression: Capturing Non-Linear Relationships\n",
    "\n",
    "Simple linear regression excels at modeling linear relationships between features and target variables. However, real-world data often exhibits non-linear trends, where a straight line wouldn't accurately capture the underlying pattern. \n",
    "\n",
    "**Polynomial Regression** addresses this challenge by introducing polynomial terms of the feature variable(s). These terms allow the model to represent curvature or bends in the data, resulting in a better fit for non-linear relationships.\n",
    "\n",
    "**Understanding Polynomial Regression:**\n",
    "\n",
    "In polynomial regression, the relationship between the independent variable `x` and the dependent variable `y` is modeled as an nth degree polynomial in `x`. Here's an example of a second-degree polynomial:\n",
    "\n",
    "\n",
    "$y = ax^2 + bx + c$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- `c` is the intercept (y-axis value when x is zero)\n",
    "- `a` and `b` are the coefficients of the quadratic and linear terms, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af89b4-4ed5-4e0e-9429-ded15bbb9016",
   "metadata": {},
   "source": [
    "**Creating Polynomial Features:**\n",
    "\n",
    "To fit our data to this equation, we need more than just the original `x` values (e.g., `weight`). We can create additional features by raising `x` to different powers. The `PolynomialFeatures()` function from `scikit-learn` automates this process. It generates a new feature set containing all polynomial combinations of the original features with degrees less than or equal to the specified value.\n",
    "\n",
    "For instance, let's say our initial feature set consists of just one feature: `weight`. If we set the degree of the polynomial to 2, then `PolynomialFeatures()` creates three features:\n",
    "\n",
    "1. Degree 0: Represents a constant term (equivalent to ***1*** in the equation)\n",
    "2. Degree 1: Linear term (***x***)\n",
    "3. Degree 2: Quadratic term (***x<sup>2</sup>***)\n",
    "\n",
    "This transformed feature set allows the model to capture non-linear relationships between the feature and the target variable more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7f882-8a14-4f8a-a14f-74a02d5d0b60",
   "metadata": {},
   "source": [
    "## Polynomial Regression Model: A Special Case of Linear Regression\n",
    "\n",
    "While polynomial regression might seem like a completely different approach compared to multiple linear regression, it's actually a special case of it! The key difference lies in how we select and create the features used in the model.\n",
    "\n",
    "**Understanding the Connection:**\n",
    "\n",
    "Imagine we take the transformed data set generated by `fit_transform` (containing features like $x$, $x^2$, etc.) for a degree-2 polynomial. We can rename these features:\n",
    "\n",
    "- $x$ (degree 1) becomes $x_1$\n",
    "- $x^2$ (degree 2) becomes $x_2$\n",
    "\n",
    "By doing this, the original degree-2 polynomial equation:\n",
    "\n",
    "$y = ax^2 + bx + c$\n",
    "\n",
    "transforms into:\n",
    "\n",
    "$y = ax_2 + bx_1 + c$\n",
    "\n",
    "This equation now resembles the structure of a multiple linear regression equation, where $x_1$ and $x_2$ are independent variables and $y$ is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c0484-100f-4044-821b-eac0f0ead9f4",
   "metadata": {},
   "source": [
    "## Creating Polynomial Features with `fit_transform`\n",
    "\n",
    "The `fit_transform` function from scikit-learn's `PolynomialFeatures` class plays a crucial role in polynomial regression. It takes our original feature data (`x`) and transforms it into a new feature set suitable for modeling non-linear relationships.\n",
    "\n",
    "**Understanding the Transformation:**\n",
    "\n",
    "Suppose we set the degree of the polynomial to 2 (quadratic). In this case, `fit_transform` performs the following actions:\n",
    "\n",
    "1. **Generates New Features:** It creates additional features by raising the original features (`x_values`) to different powers from 0 to the specified degree (2 in this case). This results in three features:\n",
    "   - A constant term (degree 0) represented by 1 (equivalent to `[1, ...]` in the output).\n",
    "   - The original feature itself (degree 1) represented by `x_values`.\n",
    "   - The squared term of the original feature (degree 2) represented by `x_values**2`.\n",
    "\n",
    "2. **Transforms the Data:** It combines these newly created features with the original features to form a new transformed data set. This transformed data set is more suitable for capturing non-linear relationships between the features and the target variable.\n",
    "\n",
    "**Visualizing the Transformation:**\n",
    "\n",
    "The mathematical representation of this transformation can be expressed as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v_1\\\\\\\\\n",
    "v_2\\\\\\\\\n",
    "\\vdots\\\\\\\\\n",
    "v_n\n",
    "\\end{bmatrix}\\longrightarrow \\begin{bmatrix}\n",
    " v_1^0 & v_1^1 & v_1^2\\\\\\\\\n",
    " v_2^0 & v_2^1 & v_2^2\\\\\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\\\\\n",
    " v_n^0 & v_n^1 & v_n^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $v_i$ represents the i<sup>th</sup> value in your original feature vector (`x_values`).\n",
    "\n",
    "For example, consider a sample data set:\n",
    "\n",
    "```\n",
    "x_values = [2265, 2515, 2220, ...]\n",
    "```\n",
    "\n",
    "Applying `fit_transform` with a degree of 2 would generate the following transformed data:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2265\\\\\\\\\n",
    "2515\\\\\\\\\n",
    "2220\\\\\\\\\n",
    "\\vdots\n",
    "\\end{bmatrix} \\longrightarrow \\begin{bmatrix}\n",
    " 1 & 2265 & 5130225\\\\\\\\\n",
    " 1 & 2515 & 6325225\\\\\\\\\n",
    " 1 & 2220 & 4928400\\\\\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As you can see, the original feature values (`2265`, `2515`, `2220`, etc.) are preserved, and additional columns containing their squares (`5130225`, `6325225`, `4928400`, etc.) are added. This expanded feature set allows the model to learn more complex relationships between the features and the target variable.\n",
    "\n",
    "By understanding how `fit_transform` works, you gain valuable insights into how polynomial regression prepares data for modeling non-linear patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b934ea-c956-4f62-890a-b740d29aeba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de6e90c-21d1-4e13-9728-1fbe96f48b5f",
   "metadata": {},
   "source": [
    "**Leveraging Linear Regression Machinery:**\n",
    "\n",
    "Because of this similarity, we can treat the transformed data set from polynomial regression as suitable input for the `LinearRegression()` function from `scikit-learn`. The model will then internally handle the relationships between the features ($x_1$, $x_2$, etc.) learned during the transformation process.\n",
    "\n",
    "In essence, polynomial regression leverages the existing machinery of linear regression to model non-linear relationships. It achieves this by creating new features that capture the curvature or bends in the data.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "Understanding polynomial regression as a special case of linear regression simplifies the modeling process. You can utilize familiar linear regression techniques while expanding your capabilities to handle non-linear data patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53367b-dee6-4832-9e38-7a53ee7a53dd",
   "metadata": {},
   "source": [
    "Absolutely, I can revise the text for polynomial regression using the insights from the simple linear regression text and your provided code snippet:\n",
    "\n",
    "**Model Building in Polynomial Regression:**\n",
    "\n",
    "1. **Polynomial Feature Creation:**\n",
    "   - The `PolynomialFeatures()` function from scikit-learn plays a crucial role. It transforms the original feature data (`x_train` in this case) to include polynomial terms up to a specified degree. This creates new features that capture non-linear relationships between features and the target variable.\n",
    "\n",
    "2. **Model Instantiation:**\n",
    "   - Similar to simple linear regression, we use `model = LinearRegression()` from scikit-learn to create a linear regression model. This model will be used to learn the relationship between the transformed features (containing polynomial terms) and the target variable (`y_train`).\n",
    "\n",
    "3. **Model Fitting:**\n",
    "   - The `model.fit(train_x_poly, y_train)` line trains the model. Here, `train_x_poly` represents the transformed training data generated by `PolynomialFeatures()`, and `y_train` is the target variable. During training, the model learns the coefficients for each feature (including the newly created polynomial terms) that best fit the data.\n",
    "\n",
    "4. **Extracting Coefficients and Intercept:**\n",
    "   - After fitting, you can access the model's coefficients using `model.coef_` and the intercept using `model.intercept_`. These values provide insights into the learned linear relationship between the features and the target variable.\n",
    "      - Coefficients represent the weight or influence of each feature (including polynomial terms) on the target variable.\n",
    "      - The intercept signifies the predicted value of the target variable when all features are zero (assuming a linear relationship between the transformed features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa2a62-1bf1-4bf6-93e9-0c77dbd50669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a131dc7a-bb0f-4497-9cbb-efe6b8bb02d2",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "- Polynomial regression builds upon linear regression by creating new features that capture non-linearity.\n",
    "- The core model fitting process remains similar to linear regression, using `LinearRegression()` for model creation and `model.fit()` for training.\n",
    "- Interpreting the coefficients and intercept helps understand the relationships between the features (including polynomial terms) and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2446c-2570-43c4-b701-9aff986f7826",
   "metadata": {},
   "source": [
    "## Model Predictions on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0a6c6-8fef-4a9d-9933-f715622f4d85",
   "metadata": {},
   "source": [
    "This section leverages the trained model to generate predictions for the unseen testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464e009-c227-4884-98d0-57fb1029304f",
   "metadata": {},
   "source": [
    "**Prediction Generation:**\n",
    "- the `model.predict(x_test)` method is used to make predictions on the testing set (`x_test`). The model applies the learned relationship between `weight` and `mpg` to predict the `mpg` values for the weight data points in the testing set. These predicted `mpg` values are stored in the `y_predict` variable.\n",
    "\n",
    "By performing predictions on the testing set, you can access the model's generalization ability and evaluate its performance on unseen data. The next step will likely focus on this evaluation using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f51ed-3749-4af5-b045-c911e7e6612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87285d8c-7aa0-434b-a758-9988b95ef567",
   "metadata": {},
   "source": [
    "## Model Evaluation - Predicted vs. Actual Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9393d-b457-4c28-bdb2-467c256d5e56",
   "metadata": {},
   "source": [
    "This section delves into evaluating the model's performance by comparing the predicted `mpg` with the actual `mpg` values in the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195d496-7c83-45d1-88bf-5cc5011ccdee",
   "metadata": {},
   "source": [
    "**1. Comparison DataFrame Creation:**\n",
    "    \n",
    "- A `pandas` DataFrame named `frame` is constructed using `pd.DataFrame({'Weight': np.squeeze(x_test), 'Real MPG': y_test, 'Predicted MPG': y_pred})`. This DataFrame combines the actual `mpg` values from the testing set (`y_test`) in a column named `Real MPG`, and the predicted `mpg` values in a column named `Predicted MPG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b6e25-42a8-44b2-adbe-0618bfc5c3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70890edd-6b8a-49ab-b205-ff0ee412c3be",
   "metadata": {},
   "source": [
    "**2. Visualization (Optional):**\n",
    "- By examining the DataFrame on creating visualizations, you can gain valuable insights into the model's ability to accurately predict `mpg` values based on vehicle `weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb0658-c216-4679-ac11-9a7a8c6e407a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f934259b-ae28-460f-8cf3-7674767af3c2",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- Additonal metrics like **mean squared error (MSE)** or **R<sup>2</sup>** can be employed for more quantitative evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13634dc-f93f-4174-b32a-31ab2e9ae58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76660a3f-ba28-45cc-99ba-556a70c5da31",
   "metadata": {},
   "source": [
    "## Visualizing the Relationship Between Weight and MPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c902c-8890-48ee-bc32-e2b9a3279726",
   "metadata": {},
   "source": [
    "This section delves into visualizing the relationship between vehicle `weight` and `mpg` using two common `matplotlib` librariy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b096cf-b23d-4141-b75e-3b181dd8436a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
